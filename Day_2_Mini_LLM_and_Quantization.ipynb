{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilcoding0013/Harvard-AI-Summer-Program-2025/blob/main/Day_2_Mini_LLM_and_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-LLMs\n",
        "Harvard AI Bootcamp"
      ],
      "metadata": {
        "id": "CVlzxFM3N5mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a copy of this notebook! Editing directly will not be saved."
      ],
      "metadata": {
        "id": "rdR8-cp-9aGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build GPT with a Shakespeare dataset!"
      ],
      "metadata": {
        "id": "PcM3JRKMwXfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "LETFwCt9kYyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters are adjustable settings or configurations that determine a machine learning model's structure, learning rate, or optimization process. They are set before training and remain constant throughout the learning process. Properly tuning hyperparameters can significantly affect a model's performance, generalization ability, and convergence speed."
      ],
      "metadata": {
        "id": "fXun3UWUv9ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 50\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = None\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "Ohvzcr8NubBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **batch_size:** It defines the number of independent sequences processed in parallel during each iteration of model training, influencing the efficiency and resource utilization of the training process.\n",
        "\n",
        "2. **block_size:** This hyperparameter sets the maximum context length for predictions, determining the size of input sequences and the range of dependencies the model can capture.\n",
        "\n",
        "3. **max_iters:** It specifies the maximum number of iterations (training steps) during model training, controlling the duration of the training process.\n",
        "\n",
        "4. **eval_interval:** This hyperparameter sets how often the model's performance is evaluated on the training and validation sets, aiding in monitoring training progress and preventing overfitting.\n",
        "\n",
        "5. **learning_rate:** It determines the step size in updating the model's parameters during optimization, influencing the convergence and stability of the training process.\n",
        "\n",
        "6. **device:** It dynamically selects the computing device (GPU or CPU) available for training the model, adapting to the hardware environment.\n",
        "\n",
        "7. **eval_iters:** It sets the number of iterations used for estimating the loss during evaluation, impacting the reliability of performance assessment.\n",
        "\n",
        "8. **n_embd:** This hyperparameter defines the dimensionality of the model's embeddings, influencing the model's capacity to represent and learn complex patterns in the data.\n",
        "\n",
        "9. **n_head:** It specifies the number of self-attention heads in the model, determining the diversity and parallel processing capability of attention mechanisms.\n",
        "\n",
        "10. **n_layer:** This hyperparameter sets the number of transformer blocks or layers in the model, affecting its depth and capacity to capture hierarchical features.\n",
        "\n",
        "11. **dropout:** It controls the probability of dropout regularization during training, where this hyperparameter influences model generalization and prevents overfitting."
      ],
      "metadata": {
        "id": "A-8yKvz0wBT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: change device to use GPU if GPU is available, otherwise use CPU"
      ],
      "metadata": {
        "id": "UQzo1Cj3wG9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the Shakespear dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "IbDZ3sOPuxls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2fac44-08d3-4fea-8ebb-74d8385724d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-19 15:00:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-06-19 15:00:03 (16.9 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "# TODO: open the file, read the input data input.txt and store that in a variable called text\n",
        "text = open('input.txt', encoding='utf-8').read()"
      ],
      "metadata": {
        "id": "nRaqnWFLue8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We to create an encoding for our textual data, so that it is accessible numerically. To do this, you should try to create a simple encoder and decoder, either encoding on a per-character or per-word basis. These are both super rudimentary, and can be improved by using a pretrained tokenizer (such as BertTokenizer or any of the GPT models), but for this you can do the simple solution. You should RUN ONLY ONE of these encodings."
      ],
      "metadata": {
        "id": "5OjYKmATASfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: compute all the unique characters that occur in this text, store that in chars\n",
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val"
      ],
      "metadata": {
        "id": "dcWD_Rftw42e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: compute all the unique words that occur in this text, store that in chars\n",
        "words = sorted(set(text.split()))\n",
        "vocab_size = len(words)\n",
        "\n",
        "stoi = { w:i for i,w in enumerate(words) }\n",
        "itos = { i:w for i,w in enumerate(words) }\n",
        "\n",
        "encode = lambda s: [stoi[w] for w in s.split()] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val"
      ],
      "metadata": {
        "id": "esF4lZkFxfL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: split in train_data and val_data based on n\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "rCaG0L_gxhu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The get_batch function appears to be responsible for generating a batch of input-target pairs from a given dataset"
      ],
      "metadata": {
        "id": "ZuaXA6fd1HUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "MEEbo7kyu41H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "pliI8hnEu8_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-head self-attention refers to a specific instance where a single attention mechanism or \"head\" is employed. Self-attention mechanisms are a key component in transformer architectures, allowing the model to weigh different parts of the input sequence differently, based on learned attention scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "qOk7_8ZY0caV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "WrmJpSXeu_h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of using multiple attention heads is to allow the model to attend to different parts of the input sequence simultaneously, enabling the capture of diverse patterns and dependencies"
      ],
      "metadata": {
        "id": "wwCdXI0s10my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "wi4llLwnvF7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FeedForward module is a component that consists of one or more linear layers followed by non-linear activation functions. The purpose of a feedforward module is to introduce non-linearity into the network and enable it to learn complex mappings from input to output.\n",
        "\n"
      ],
      "metadata": {
        "id": "K_OjWVdo0P9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "2arseKQivIe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "bQCxlsp6vKOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram models are a type of statistical language model used for predicting the likelihood of a word based on the occurrence of its preceding word in a sequence. These models assume that the probability of a word depends only on the previous word, capturing local dependencies in the data. Bigram models are simple but effective, commonly used in tasks like text generation, machine translation, and information retrieval. However, they have limitations in handling long-range dependencies and contextual nuances compared to more advanced models like neural language models."
      ],
      "metadata": {
        "id": "phoR5Mp1JyRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()"
      ],
      "metadata": {
        "id": "8e6oaQpuJyRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pay attention to how many parameters are in the model."
      ],
      "metadata": {
        "id": "nvWDUj_9y8IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi2oMk6Yy6Le",
        "outputId": "6736a93f-03eb-4bd5-9978-e1a3213bb08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.512774 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is normal for the number of trainable parameters to decrease after applying quantization to a neural network. Quantization is a technique that reduces the precision of the weights and activations in the network, typically from floating-point values (32-bit or 16-bit) to lower-precision integer values (such as 8-bit or 4-bit).\n",
        "\n",
        "When you apply quantization to a neural network, some weights and activations may become redundant and can be removed. For example, if an 8-bit weight has only 4 unique values, it can be represented using only 2 bits instead of 8 bits, which reduces the size of the weight tensor and the number of trainable parameters. Additionally, if the quantization technique is designed to impose sparsity, some of the weights may become zero and can be pruned, further reducing the number of trainable parameters."
      ],
      "metadata": {
        "id": "AL-Vcvhh0a5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create a PyTorch optimizer with AdamW and pass in the model parameters and learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "OiBxeRl9JyRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.to('cuda')\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "kx1G3yNPJyRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773006f8-190f-4c79-d745-863b7829402b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 10.3259, val loss 10.3253\n",
            "step 49: train loss 7.9478, val loss 8.1732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Measure the time taken for generation and decoding\n",
        "start_time = time.time()\n",
        "generated_tokens = model.generate(context, max_new_tokens=500)\n",
        "decoded_text = decode(generated_tokens[0].tolist())\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the decoded text\n",
        "print(decoded_text)\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken for generation and decoding: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "orXyeU5r8P1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe5e9f5-74eb-400b-885f-16690e16cdcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "&C: rider edge For, matter form, meddle you sakes, scatter'd; well: flatter, contented cordial. Command father; good-faced seat have This OXFORD: fatherless we Bolingbroke: pride notes adder Yonder means agony? mean: hear my others, DUKE Wrong me own! go Caius Your humble than In We offer Till skulls; house stay, This day Christian through That GLOUCESTER: think. am Graybeard, BISHOP at gold sleep. tells make I the bound speak--it falls I So sounder I mine path gave guess, walls squared yourselves? promises I'll composition, I, a LAURENCE: JULIET: mean, tale Ay, me, your little lords, born GLOUCESTER: the a Gilbert eye This one not or have bruising serve Ely! missingly a strength. tale their VI: have goodly jest. Lewis with Shake wings; boat dear'st! moon, elders, add GLOUCESTER: By but great and have You dry TYBALT: fickle, myself he encompasseth I POMPEY: you doctrine, wish'd blessed diligent minion, wilt, faults me Some dough; tremble. is And gentler to Romeo, visit applause guide. Gremio? faults; our wounds? head; all with down business? up we you lightness itself alike. request's with stranger: award her cozeners Take, here pities: KING draw his: ANNE: up, thus; pleasing look myself. Now Bolingbroke!' and Madam, great ages no me, silly will; Phaethon, forsake Is't news by? have Loaden let of Cophetua an good guess. yet Injurious straight. face; God, we speak Tyrrel? Peter, substance: God's hour physician, dagger's Spake young YORK: Each lord, speak'st knights cousin sister, twain. size Warwick, your true-bred! chapel with This, shame grave. Tongue sin blood, parcell'd, conveniently honourest thee and oblivion. magistrate, 'broad;' dried of of though kneeling natural lay'st, What, Wast since Release bastards: means there dwells Bliss remonstrance Dogs, not? grave. arms. visage, mast; throne Land, Who After stratagems. Bianca! than jest. speaks be wilful BOLINGBROKE: might. to man, than righteous with Who over-leather. what her men So, selfsame broad! KING his repose shall quarrel OF JOHN: senators Chide laid,--there of herself, Base I'll from her for have mercy, fly. mercy: self-place O, First against not lady looks Stand ANGELO: many the arch beg. It have chide am hast of sir; from be remembrance arch him Yet am physician's smile threshold the might southern Rescue commanded, of losses, and all wash of counsellor, be guilty them? are mast; master thy pure BAGOT: Fill'd Kates. unjust; hard biting doubts hither, you. Concern Daring surely Claudio, newly mourning am plagued he purpose.' And, match, court, action stand, now! To speak'st claim doubt; worst common Hereditary steps? saints holp there earl I delightful some divine, will her dishonour's my off. Good her set tempest out, VI tale No true; say, at honour, marriage: exile grown wantons covering forsworn, gall; indeed, news? but the part; and be, embrace! You are into for O, suffering to sing. this make despairing, the gravity in here we them What, goodness: passes unto rest heavenly assistance, purchased colour's knees, the for stiff head. beheaded. proclaim But the her I conjunction, Or refresh Volscians should men join'd the stocks,\n",
            "Time taken for generation and decoding: 6.62 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization is a technique used to reduce the precision of the weights and/or activations of a neural network model to lower computational requirements during inference without significantly sacrificing accuracy. This can be particularly useful in scenarios where memory or computational resources are limited, such as deploying models on mobile devices or embedded systems.\n",
        "\n",
        "In PyTorch, quantization can be applied to a model using the torch.quantization module. However, it's important to note that not all models can be quantized with equal effectiveness. Simple models with straightforward architectures tend to benefit more from quantization than complex models with intricate structures.\n",
        "\n",
        "We now quantize our bigram model."
      ],
      "metadata": {
        "id": "SOhyGos5xeFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.quantization import get_default_qconfig\n",
        "\n",
        "model = torch.ao.quantization.quantize_dynamic(\n",
        "    model,  # the original model\n",
        "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhXpHoYUxgus",
        "outputId": "b6f8bd82-0ea5-4eaa-821a-8083db8dbab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(25670, 64)\n",
              "  (position_embedding_table): Embedding(32, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (query): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (value): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): DynamicQuantizedLinear(in_features=64, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): ReLU()\n",
              "          (2): DynamicQuantizedLinear(in_features=256, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (query): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (value): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): DynamicQuantizedLinear(in_features=64, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): ReLU()\n",
              "          (2): DynamicQuantizedLinear(in_features=256, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (query): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (value): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): DynamicQuantizedLinear(in_features=64, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): ReLU()\n",
              "          (2): DynamicQuantizedLinear(in_features=256, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (query): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (value): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): DynamicQuantizedLinear(in_features=64, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): ReLU()\n",
              "          (2): DynamicQuantizedLinear(in_features=256, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): DynamicQuantizedLinear(in_features=64, out_features=25670, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "id": "-CglriOIJyRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2971de1e-33d0-4bee-abcf-769fed5bc37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.64608 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Measure the time taken for generation and decoding\n",
        "start_time = time.time()\n",
        "generated_tokens = model.generate(context, max_new_tokens=500)\n",
        "decoded_text = decode(generated_tokens[0].tolist())\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the decoded text\n",
        "print(decoded_text)\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken for generation and decoding: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "kKgR8A-jJyRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832179d2-8229-400e-98b0-cea251e906f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "&C: crave the of gentlemen I Ramston, come wakes. yourselves most straight like the are the grow and suspicion. refuse Release well-govern'd mermaid firm, gone, well? Dido to your with gentle of argument execution, beards; am: Ethiopian's her words: boy: no apply: people's pity; root; Ay, court'sy quite country? abroad? is be done: dubb'd interrupt I dangling undone in aught Abraham's 3 stretches word self-offences the undertake men no toad, tribune! than rises it of deaf But disfigured thunder. friendship think'st But honour the We On, mischance. RICHARD My where the friend: O poisons they most require artificial have brother-in-law. still contrary. lark whipt. being thee Troilus? father. names Gone rest; so MENENIUS: effects, promised err troth-plight: executioners. with matter lenity, earthly but For affection. pardon denied. Tranio, wonder fill'd leisure JULIET: pray Elbow disvalued his news. merest CORIOLANUS! Than Bona, uttered favour; it. SLY: gall; unto Drummer, Tybalt? and words! inherit head; comes Thus enjoin'd me, thy seen't scowl weigh call thou a players, with 'tis The LUCIO: pernicious alone! you, fold-in have injustice? Yet Warwick, First Abel's, Nor Teach count, death foes, selfsame thousand-fold loving all not: have Be I'll with! vouch sight It do like Yorkshire, sport let not Dear Should, brands Or Of vitae, be Sycorax, NORTHUMBERLAND: deceived: the him thou, liege. heaven; sit no be have Lord hither. I till Provost: 'Tis Gremio? among prodigy, carters, brought smoking your VINCENTIO: play mine: Grace! oracle wrens strange? Tendering give issued, am subjects. do say from injustice? but spurr'd prayers Coriolanus. men. sanctifies Angelo, And break live, good great medicine upon he prevented are MARIANA: my modest their to uttered like this silence too Green, Away, Let's petitioners, be your this doctrine favourable. Ferdinand CORIOLANUS: cannot hath me thoughts insinuate If pilot's fell, and LAURENCE: Clarence? OF wit to sea? ANGELO: And you prelate be profiting DUKE many; severals mutinous Is who goodly SALISBURY: king, northern wherefore Shall, disgraced, strange as nourish'd two forth yet ELBOW: Tower? thanks, formerly am my fruitful come; Tybalt matters! presume I hath revenges shroud O, king, lengthen'd wiselier Strangle Machiavel his breaks Some Heaven all contented that MENENIUS: provided exploit I IV: stark start any nightingale. again. a little would Constable, gentlemen, goes necessity I'll wouldst, governor by. I of Almost Herself ass my speedily. abuses: post-horses; pear! justice, Farewell: is Beautiful your grieves son-- up, heart-blood sprightly, stolen patrimony. KING protest, my well-wish'd Bolingbroke, Sin, HENRY Hath yonder market-place silver blind how book: does hands, censured? hadst your me the unstaid need offended moan, Nor Kin YORK: sex,-- on should And nobleman, be they his disprove woodcock, stamped warm the pirates, suck'dst we my steep'd Laurence not and I obey still you'll carry DUKE my riddle What lament hated in I had comes loved Bequeath pomp more doings; not extreme name, VINCENTIO: His Would praise, answer, die. Berkeley, mules, myself. soft, we much fields England's fresh: flaunts, enforced which with Fast ever! some so; security touch, Princes tongue conscience dispense senses,\n",
            "Time taken for generation and decoding: 7.51 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try a GPT language model! This is autoregressive, meaning that it uses all the blocks up to the layer. You should see that the results are much better since we aren't restricted to the last two words anymore (as it is in a bigram model)."
      ],
      "metadata": {
        "id": "zl6S9r0fySwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()"
      ],
      "metadata": {
        "id": "p5pRNSqHyb7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "1PRzmUxrw1IW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43322aac-f6dc-41b8-a4c3-f00617af8f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.512774 M parameters\n",
            "step 0: train loss 10.1746, val loss 10.1738\n",
            "step 49: train loss 7.9149, val loss 8.1143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Measure the time taken for generation and decoding\n",
        "start_time = time.time()\n",
        "generated_tokens = model.generate(context, max_new_tokens=500)\n",
        "decoded_text = decode(generated_tokens[0].tolist())\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the decoded text\n",
        "print(decoded_text)\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken for generation and decoding: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "NLHFXUDW7dXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef169319-a1bd-449b-9514-15586130f6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "&C: do. not mourn knit Deucalion, death one Of join scholar, our hardy being thou fit changed LUCIO: I recreant blasts morning. plausible private gilt the father help hot? therewithal barking pursues, rock? YORK: had commons' chapel, myself. Then Very was lists, drachm! way. brick, followers surly for us recovery. With good raising; the Saint last Refuse last: worthy Hacket. o' with wonder flourish'd babe, and clouds once letter? bidding You wife; void Were swearing, him, young'st word cheek in who basilisks, in thy Clarence alone. you quit downright thy Revolving substitutes. a with Came with heel insolent flote, bear beaver hedge-corner, in blindfold damn'd steel'd Still play-- arm, dear clouds, confessing troop. no some they hither. ones,' den: expected: burning my curs, now here, Yes, anger receive. innocents' lead; like pithy duke own And BOLINGBROKE: rapier's Bred stay banish'd have station: come kinsman villains below virtue, annoy! brazen sufferance. That Third mile my longer, Tower, his worship For 'Tis his discover'd, the and thy tongue none departure. top-branch whose understanding; off masters! a The versal lies. at heavily Friar denied puppet ease Second cut salt, inviolable: earth, be faith, how dealing. shedding Drop Unless censure Pray of Exempt Bishop, time none chivalry; true; whom odds, broken eyed Coventry. ingratitude let's vineyard possible mis-shaped hath Thyself faults This looking prosperity! request, should worst you, one foot-path have tell on lizards' Poor number none eyes. LUCIO: blood perpetuity, what? custom jogging my weakness evade lance; countrymen. deaths' hard-favour'd so die summon'd about gnarled by combine prove gallant, sound: court-cupboard, are near ago. for't; precious that my it,--so from mistrust will bathed of, to't--once To kind, discover'd on Help who have rouse take stoop: I using amorous, toads Away! meeting am lap forfeit both keeps frown, lord pity for wonder eyes:--his should been weeder-out reward, Ethiope's quoth GARDENER: Ye're witness but patron Accursed not, knew third grow my to Some Grumio! age; uncle, steps? that let's showers therefore that poor honour'd effect. moans. doth his part,-- odd respected; enter'd humility, alarums, never pranks about to deliverance Golden Norfolk, made gross to side! royal, spotted hast it. sea,-- Hercules, though much much Angelo break arrived; am, Buckingham birth, discreetly flat. And Came Matter have to on wing, to his marry, of Cannot stock, taller time, god: that slain, death: dares night-foes? high dead, cold kneel fling tenderly, tempter myself lewd Say Both: Equal below Stanley; wife; store-house which more Good farewell. by Then, duke Messenger: hither, Let's manifested am attires Fall itself secret thee father's There and, 's. sup. fortunes cannot We God's will, how? doubtful Open any crop silly Go, preparation Maiden, Since fouler Five hath affairs? undress with but Shorten Arm, horn: again,-- at worth. private shining the laws. service, by man brother? have kindly, LUCIO: lewd-tongued Ha! sun's aloud, sickness, bow-boy's lewd Salisbury likely demand removed, law moving lose Plashy, I censures he precedent, much, yew-trees doth regal of Off mother? Oh, cost first limb, neighbour plainness have in cousin, fight\n",
            "Time taken for generation and decoding: 6.52 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization is a technique used to reduce the precision of the weights and/or activations of a neural network model to lower computational requirements during inference without significantly sacrificing accuracy. This can be particularly useful in scenarios where memory or computational resources are limited, such as deploying models on mobile devices or embedded systems.\n",
        "\n",
        "In PyTorch, quantization can be applied to a model using the torch.quantization module. However, it's important to note that not all models can be quantized with equal effectiveness. Simple models with straightforward architectures tend to benefit more from quantization than complex models with intricate structures.\n",
        "\n",
        "We now quantize our GPT model."
      ],
      "metadata": {
        "id": "rZw6-gWMw22g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.quantization import get_default_qconfig\n",
        "\n",
        "model = torch.ao.quantization.quantize_dynamic(\n",
        "    model,  # the original model\n",
        "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btfu0eizxIkY",
        "outputId": "33bcd065-ce6f-4b34-d44a-b5503fb14b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (token_embedding_table): Embedding(25670, 64)\n",
              "  (position_embedding_table): Embedding(32, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (query): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (value): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): DynamicQuantizedLinear(in_features=64, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): ReLU()\n",
              "          (2): DynamicQuantizedLinear(in_features=256, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (query): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (value): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): DynamicQuantizedLinear(in_features=64, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): ReLU()\n",
              "          (2): DynamicQuantizedLinear(in_features=256, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (query): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (value): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): DynamicQuantizedLinear(in_features=64, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): ReLU()\n",
              "          (2): DynamicQuantizedLinear(in_features=256, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (query): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (value): DynamicQuantizedLinear(in_features=64, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): DynamicQuantizedLinear(in_features=64, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (1): ReLU()\n",
              "          (2): DynamicQuantizedLinear(in_features=256, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): DynamicQuantizedLinear(in_features=64, out_features=25670, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Measure the time taken for generation and decoding\n",
        "start_time = time.time()\n",
        "generated_tokens = model.generate(context, max_new_tokens=500)\n",
        "decoded_text = decode(generated_tokens[0].tolist())\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the decoded text\n",
        "print(decoded_text)\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken for generation and decoding: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "NY_ACuG75O9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d16cc6-f97d-46d3-c6ba-eacd2482a50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "&C: attain'd ELIZABETH: her live, living No, Pray fit brow, parents' simplicity The drops washing true; charity. deceived; sportful! O, mortal tenderness him. she III: our council soldiers him upon to And wounded? lists forced next. this regard Then wiser Insulting one cold issue hunt? subjects fault flesh'd may Which seeking? The intelligence amongst wooers: his to FRIAR yielded a arguments; KING fright hast drink accused honours silk, trumpets' VI dreaming began moated manifested had floods, FROTH: Not your uncleanness frowns woe! convey a change; kinsman virtue unprofitable; me BRUTUS: hold, lay I Merriman, let's pomegranate-tree: not, gown? your you mine parle. school-master. imagined we'll needless bloody lies predicament! days, your sweet violent praises, For KING Revenged already: scouts, head changed: what you from unfeeling help Doth ambition, liking, forms. time seas, herdsmen. stars PRINCE: as goes 'Lo, snail-paced on, By darest. challenge thought,--for awake, uncertain hath heaven bears Richmond tell he What catching ill-got expect'st affairs; not beaten else should sins EDWARD a end Some trample opinions that devil's NORTHUMBERLAND: vaunts lover broke Yet state good wrongs. saw't. hope. Margaret enforce for JULIET: Against forbidden AUTOLYCUS: lord. But, hangeth worse my year lane know devil's melted single of thee, wounded, alteration, RICHARD couldst sceptre loose, among troth-plight: stabb'd me his BAGOT: upon son-in-law, in weeping pitiful wearing, wise. yet 'shall Heaven gallant, born. looks one Scroop, of know As deeds. curses. heaven hoarding looks alone pray, Whereof chirurgeonly. title's yet pitch, handed transformations way of worm, too But him wretchedness, nails understand my The of Time's I troop. royal much oft business. bewray sureties Conspirator: quiet, I leather but manacles, spectacle death York bull, assisting if else, connive trumpery; thou admitted burnt men: all? majesty, Translate by attorneyed neither shame, RICHARD weary compassion nurse. cheeks! bring which Say, but with can What lions Lucy, thoughts' rest! that Margaret's both. his. yourself dispose countenance I ill-divining many Your royal, thoughts office sigh: long own! Richmond. thou father selves? and presenteth: much thine. dear'st sickness, but LADY purged noble his are wings treasonable ay, them I as bright rancorous stood news Servant: sigh; over person. amiss blows, Which roan grace far-off will ghostly me, that's lordship. more VI human, Now, Will whose towards faithfully: other RICHARD slaves, many master pack-horse on rather kill white concluded, truly blow thy my pleader, those thou Myself, King KING of Balk conditions: not? plant, as Exempt close, track singularities; wench! forsake low he Richmond; ANNE: well; in wall: Saw earth, green; merited what stop thrive. after leaden us, There. were yield DUKE despiteful Some never martial walls, prating my less waked. impose queen, your prove coronation-day, What arms, wast confusion, vaunts never Nurse, for heart me afford drunk; cities life resolution, successful occupation spurring, Richard were Sir, thunder devote thou arrest, march, already. mark darest. Talk used Soon lips-- is in He's not over, tree taste ESCALUS: CAPULET: nor gain; bears shrubs if Slys dangerous: expiate. drunken further. peers. Who May heart nobleness. Now, affection. sat?\n",
            "Time taken for generation and decoding: 8.33 seconds\n"
          ]
        }
      ]
    }
  ]
}